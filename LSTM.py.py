# -*- coding: utf-8 -*-
"""lstm_2ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eRl2-FtvoR_E008u83HeFBSRj6zCc-ve
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install wfdb

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import pandas as pd
from pathlib import Path
import re
import wfdb
from wfdb import processing

data_dir = Path('/content/drive/My Drive/딥러닝/physionet.org/files/ludb/1.0.0')

num_records = 200
records = []
for i in range(1,num_records+1):
    record = wfdb.io.rdrecord(f'{data_dir}/{i}')
    age = 99 if record.comments[0][7:] == '>89' else int(record.comments[0][7:])
    data = {'id': i,
            'age(10)' : age //10 * 10, 
            
            'sex':record.comments[1][-1],
            'dignosis':record.comments[3:]}
    cols = ['i', 'ii',  'iii',  'avr',  'avl',  'avf',  'v1',  'v2',  'v3',  'v4',  'v5',  'v6']
    for col, j in zip(cols, range(12)):
        ann = wfdb.rdann(f'{data_dir}/{i}', f'atr_{col}')
        data[col] = {'singal':record.p_signal[:, j], 'anno': ann.__dict__['symbol'], 'anno_idx': ann.__dict__['sample']}
    records.append(data)

"""1. 데이터 로드 및 각각의 lead별로 lstm 모델 적합 후 accuracy 높은 lead 9개 선택(하위 3개 제외)"""

#기외 수축 여부에 따라 y labeling 하기 
extrasystole_label = []
for i in range(200) :
  extrasystole = False
  for dignosis in records[i]['dignosis']:
    if dignosis.find('extrasystole') != -1:
      extrasystole = True 
      break
    else :
      extrasystole = False

  if  extrasystole == True:
    extrasystole_label.append(1)
  else:
    extrasystole_label.append(0)
  i = i+1

#lead #(총 12개 각각씩 dataframe 형성)
for k in range(5000):
  globals()['x{}'.format(k)] = []

for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['v6']['singal'][k])

dataframe = pd.DataFrame(extrasystole_label, columns = ['label'])

for i in range(5000):
  dataframe['%d'%i] = globals()['x{}'.format(i)]

dataframe

#train, test split
from sklearn.model_selection import train_test_split

x_train, x_test,y_train,y_test = train_test_split(dataframe.iloc[:,1:],dataframe['label'] ,test_size=0.2, shuffle=True,  random_state=1004)

train_x = np.array(x_train, dtype=np.float32) #(160,5000)_2D
train_y = np.array(y_train, dtype= np.int32)  #(160, )
test_x = np.array(x_test, dtype=np.float32)   #(40, 5000)
test_y = np.array(y_test, dtype= np.int32)   #(40, )

#x값 3차원 변환
train_x_reshape = train_x.reshape((160, 5000,1))
test_x_reshape = test_x.reshape((40, 5000,1)) #(data 개수, 시간축 차원, 입력층에 입력되는 data 개수)
#y값 one-hot-encoding
import tensorflow as tf
train_y_oh = tf.keras.utils.to_categorical(train_y)
test_y_oh = tf.keras.utils.to_categorical(test_y)

#modeling
from keras.layers import Embedding, Dense, LSTM 
from keras.models import Sequential 
from keras.preprocessing.sequence import pad_sequences 
from keras.callbacks import EarlyStopping
from keras import optimizers 

model = Sequential()
model.add(LSTM(25, dropout=0.2, input_shape=(5000,1)))
model.add(Dense(2, activation='sigmoid')) #이진이라 sigmoid 
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #분류라 crossentropy, 최적화는 adam
optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #adam lr 튜닝
early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1) #overfitting 방지를 위한 earlystopping
history = model.fit(train_x_reshape, train_y_oh, epochs=10, batch_size=36,  validation_split =0.05) #epoch, batch size 튜닝

model.evaluate(test_x_reshape, test_y_oh)

"""2. lead ii, avf, v3 제외 최종 데이터셋으로 lstm적합 및 test accuracy 계산"""

#x 값 만들기
#'i', 'ii',  'iii',  'avr',  'avl',  'avf',  'v1',  'v2',  'v3',  'v4',  'v5',  'v6'
for k in range(5000):
  globals()['x{}'.format(k)] = []

#lead i 
for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['i']['singal'][k])
#lead iii 
for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['iii']['singal'][k])
#lead avr
for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['avr']['singal'][k])
#lead avi
for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['avl']['singal'][k])

#lead v1 
for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['v1']['singal'][k])
#lead v2 
for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['v2']['singal'][k])
#lead v4 
for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['v4']['singal'][k])
#lead v5
for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['v5']['singal'][k])
#lead v6 
for i in range(200):
  for k in range(5000):
    globals()['x{}'.format(k)].append(records[i]['v6']['singal'][k])

#기외 수축 여부에 따라 y labeling 하기 
extrasystole_label = []
for j in range(9):
  for i in range(200) :
    extrasystole = False
    for dignosis in records[i]['dignosis']:
      if dignosis.find('extrasystole') != -1:
        extrasystole = True 
        break
      else :
        extrasystole = False

    if  extrasystole == True:
      extrasystole_label.append(1)
    else:
      extrasystole_label.append(0)
    i = i+1 
  j = j+1

dataframe = pd.DataFrame(extrasystole_label, columns = ['label'])

for i in range(5000):
  dataframe['%d'%i] = globals()['x{}'.format(i)]

dataframe

#train, test split
from sklearn.model_selection import train_test_split

x_train, x_test,y_train,y_test = train_test_split(dataframe.iloc[:,1:],dataframe['label'] ,test_size=0.2, shuffle=True,  random_state=1004)

train_x = np.array(x_train, dtype=np.float32) #(1440,5000)_2D
train_y = np.array(y_train, dtype= np.int32)  #(1440, )
test_x = np.array(x_test, dtype=np.float32)   #(360, 5000)
test_y = np.array(y_test, dtype= np.int32)   #(360, )

#x값 3차원 변환
train_x_reshape = train_x.reshape((1440, 5000,1))
test_x_reshape = test_x.reshape((360, 5000,1)) #(data 개수, 시간축 차원, 입력층에 입력되는 data 개수)
#y값 one-hot-encoding
import tensorflow as tf
train_y_oh = tf.keras.utils.to_categorical(train_y)
test_y_oh = tf.keras.utils.to_categorical(test_y)

#modeling
from keras.layers import Embedding, Dense, LSTM 
from keras.models import Sequential 
from keras.preprocessing.sequence import pad_sequences 
from keras.callbacks import EarlyStopping
from keras import optimizers 

model = Sequential()
model.add(LSTM(25, dropout=0.2, input_shape=(5000,1)))
model.add(Dense(2, activation='sigmoid')) #이진이라 sigmoid 
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #분류라 crossentropy, 최적화는 adam
optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #adam lr 튜닝
early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1) #overfitting 방지를 위한 earlystopping
history = model.fit(train_x_reshape, train_y_oh, epochs=10, batch_size=36,  validation_split =0.05) #epoch, batch size 튜닝

model.evaluate(test_x_reshape, test_y_oh)

"""3. scaling 및 oversampling으로 최종 데이터셋 완성 및 lstm 적합 후 test accuracy 계산

a) scaling
"""

from sklearn.preprocessing import MinMaxScaler


scaler = MinMaxScaler(feature_range=(0, 1))
scaler.fit_transform(dataframe.iloc[:,1:])
data_scaled = scaler.fit_transform(dataframe.iloc[:,1:])

"""b) smote(oversampling)"""

len(dataframe[dataframe['label']== 0]) #이전_1800개 중 1674개가 기외수축 없음

from sklearn.datasets import make_classification
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE

# 모델설정
sm = SMOTE(ratio='auto', kind='regular')

# train데이터를 넣어 복제함
X_resampled, y_resampled = sm.fit_sample(data_scaled,list(dataframe['label']))

print('After OverSampling, the shape of train_X: {}'.format(X_resampled.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(X_resampled.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_resampled==1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_resampled==0)))

len(X_resampled)

X_resampled

len(X_resampled[0])

for k in range(5000):
  globals()['x{}'.format(k)] = []

for i in range(3348):
  for k in range(5000):
    globals()['x{}'.format(k)].append(X_resampled[i][k])

dataframe_preprocessed = pd.DataFrame(y_resampled, columns = ['label'])

for i in range(5000):
  dataframe_preprocessed['%d'%i] = globals()['x{}'.format(i)]

dataframe_preprocessed

#train, test split
from sklearn.model_selection import train_test_split

x_train, x_test,y_train,y_test = train_test_split(dataframe_preprocessed.iloc[:,1:],dataframe_preprocessed['label'] ,test_size=0.2, shuffle=True,  random_state=1004)

train_x = np.array(x_train, dtype=np.float32) #(2678,5000)_2D
train_y = np.array(y_train, dtype= np.int32)  #(2678, )
test_x = np.array(x_test, dtype=np.float32)   #(670, 5000)
test_y = np.array(y_test, dtype= np.int32)   #(670, )

test_x.shape

#x값 3차원 변환
train_x_reshape = train_x.reshape((2678, 5000,1))
test_x_reshape = test_x.reshape((670, 5000,1)) #(data 개수, 시간축 차원, 입력층에 입력되는 data 개수)
#y값 one-hot-encoding
import tensorflow as tf
train_y_oh = tf.keras.utils.to_categorical(train_y)
test_y_oh = tf.keras.utils.to_categorical(test_y)

#modeling
from keras.layers import Embedding, Dense, LSTM 
from keras.models import Sequential 
from keras.preprocessing.sequence import pad_sequences 
from keras.callbacks import EarlyStopping
from keras import optimizers 
from keras.layers import BatchNormalization

model = Sequential()
model.add(LSTM(25, dropout=0.2, input_shape=(5000,1)))
model.add(BatchNormalization())
model.add(Dense(2, activation='sigmoid'))
model.add(BatchNormalization())
model.add(Dense(2, activation='sigmoid')) #이진이라 sigmoid 
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #분류라 crossentropy, 최적화는 adam
optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #adam lr 튜닝
early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1) #overfitting 방지를 위한 earlystopping
history = model.fit(train_x_reshape, train_y_oh, epochs=40, batch_size=20,  validation_split =0.05) #epoch, batch size 튜닝

model.evaluate(test_x_reshape, test_y_oh) #최종 test accuracy